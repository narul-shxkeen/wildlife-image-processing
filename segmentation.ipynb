{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51ef652",
   "metadata": {},
   "source": [
    "# Create SAM ‚Üí YOLOv12 Dataset\n",
    "\n",
    "This notebook samples images from the `dataset/` folder, runs the Segment Anything Model (SAM) to generate masks, extracts bounding boxes from masks, maps species labels from `image_categories_cleaned.json` to integer class IDs, and writes a YOLOv12-style dataset into an `output/` folder.\n",
    "\n",
    "Run cells in order. If you need to install dependencies, run:\n",
    "%pip install torch torchvision opencv-python pillow numpy tqdm\n",
    "%pip install git+https://github.com/facebookresearch/segment-anything.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb5797",
   "metadata": {},
   "source": [
    "## ‚ö° GPU Acceleration Setup\n",
    "\n",
    "**Important**: This notebook requires PyTorch with CUDA support to run fast on GPU.\n",
    "\n",
    "If you see `CUDA NOT AVAILABLE` below, you need to install the CUDA version of PyTorch:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "Or for CUDA 12.1:\n",
    "```bash\n",
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "**Expected speed**:\n",
    "- GPU (CUDA): ~3-8 seconds per image ‚ö°\n",
    "- CPU only: ~30-60 seconds per image üêå\n",
    "\n",
    "Run the cells below to check your GPU status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3266a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports and helpers ready\n"
     ]
    }
   ],
   "source": [
    "# Imports and helper functions\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Attempt to import SAM (segment_anything). If this fails, install in kernel's env:\n",
    "try:\n",
    "    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "except Exception as e:\n",
    "    print('Warning: failed to import segment_anything. Install with:')\n",
    "    print('%pip install git+https://github.com/facebookresearch/segment-anything.git')\n",
    "    raise\n",
    "\n",
    "def xyxy_to_yolo(box, img_w, img_h):\n",
    "    \"\"\"Convert [x_min, y_min, x_max, y_max] to YOLO format [x_center, y_center, width, height] (normalized).\"\"\"\n",
    "    x_min, y_min, x_max, y_max = box\n",
    "    x_center = (x_min + x_max) / 2.0\n",
    "    y_center = (y_min + y_max) / 2.0\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    return [x_center / img_w, y_center / img_h, width / img_w, height / img_h]\n",
    "\n",
    "def bbox_center(bbox):\n",
    "    \"\"\"Return (x_center, y_center) of a bbox [x_min, y_min, x_max, y_max].\"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    return ((x_min + x_max) / 2.0, (y_min + y_max) / 2.0)\n",
    "\n",
    "def distance_to_image_center(bbox, img_w, img_h):\n",
    "    \"\"\"Compute Euclidean distance from bbox center to image center.\"\"\"\n",
    "    img_center_x = img_w / 2.0\n",
    "    img_center_y = img_h / 2.0\n",
    "    bbox_cx, bbox_cy = bbox_center(bbox)\n",
    "    return ((bbox_cx - img_center_x) ** 2 + (bbox_cy - img_center_y) ** 2) ** 0.5\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Imports and helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d320aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU/CUDA Status ===\n",
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: False\n",
      "‚ö†Ô∏è CUDA NOT AVAILABLE - will run on CPU (very slow)\n",
      "Possible reasons:\n",
      "  1. PyTorch CPU-only version installed\n",
      "  2. CUDA drivers not installed\n",
      "  3. GPU not detected\n",
      "\n",
      "To fix, install PyTorch with CUDA:\n",
      "  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability and GPU info\n",
    "print('=== GPU/CUDA Status ===')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'GPU device: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "    print(f'Current device: {torch.cuda.current_device()}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è CUDA NOT AVAILABLE - will run on CPU (very slow)')\n",
    "    print('Possible reasons:')\n",
    "    print('  1. PyTorch CPU-only version installed')\n",
    "    print('  2. CUDA drivers not installed')\n",
    "    print('  3. GPU not detected')\n",
    "    print('\\nTo fix, install PyTorch with CUDA:')\n",
    "    print('  pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10795534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping GPU test - CUDA not available\n"
     ]
    }
   ],
   "source": [
    "# Test GPU by moving a tensor to CUDA\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        test_tensor = torch.randn(100, 100).cuda()\n",
    "        print(f'‚úÖ Successfully created tensor on GPU: {test_tensor.device}')\n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Failed to use GPU: {e}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è Skipping GPU test - CUDA not available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2273563b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to determine the device handle for GPU0: 0000:01:00.0: Unknown Error\n",
      "No devices were found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74fdc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels from image_categories_cleaned.json...\n",
      "\n",
      "‚úÖ Found 6474 images with the 7 target species\n",
      "\n",
      "Species distribution:\n",
      "  Himalayan goral: 1984 images\n",
      "  Himalayan tahr: 1592 images\n",
      "  Himalayan gray langur: 1526 images\n",
      "  Common leopard: 784 images\n",
      "  Rhesus macaque: 326 images\n",
      "  Leopard cat: 197 images\n",
      "  Yellow-throated marten: 101 images\n",
      "\n",
      "‚úÖ Randomly selected 6474 images\n",
      "\n",
      "Copying images to yolomodelimages/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6474/6474 [00:24<00:00, 264.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully copied 6474 images to yolomodelimages/\n",
      "\n",
      "Folder ready for upload to HPC server!\n",
      "Total size: 10444.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Create yolomodelimages folder with 1200 random images from 7 classes\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Configuration\n",
    "DATASET_DIR = Path('dataset')\n",
    "LABELS_JSON = Path('image_categories_cleaned.json')\n",
    "OUTPUT_FOLDER = Path('yolomodelimages')\n",
    "NUM_SAMPLES = 6474\n",
    "SEED = 42\n",
    "\n",
    "# The 7 species classes\n",
    "ALLOWED_SPECIES = {\n",
    "    \"Common leopard\",\n",
    "    \"Himalayan goral\",\n",
    "    \"Rhesus macaque\",\n",
    "    \"Himalayan gray langur\",\n",
    "    \"Himalayan tahr\",\n",
    "    \"Yellow-throated marten\",\n",
    "    \"Leopard cat\"\n",
    "}\n",
    "\n",
    "print(f'Loading labels from {LABELS_JSON}...')\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "# Filter images that contain at least one of the allowed species\n",
    "filtered_images = []\n",
    "species_counts = {sp: 0 for sp in ALLOWED_SPECIES}\n",
    "\n",
    "for img_name, categories in labels_data.items():\n",
    "    # Skip if categories is None or empty\n",
    "    if not categories:\n",
    "        continue\n",
    "    \n",
    "    # Check if this image contains any of our 7 species\n",
    "    img_species = set(categories) & ALLOWED_SPECIES\n",
    "    if img_species:\n",
    "        img_path = DATASET_DIR / img_name\n",
    "        if img_path.exists():\n",
    "            filtered_images.append(img_name)\n",
    "            # Count species occurrences\n",
    "            for sp in img_species:\n",
    "                species_counts[sp] += 1\n",
    "\n",
    "print(f'\\n‚úÖ Found {len(filtered_images)} images with the 7 target species')\n",
    "print('\\nSpecies distribution:')\n",
    "for species, count in sorted(species_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f'  {species}: {count} images')\n",
    "\n",
    "# Sample random images\n",
    "random.seed(SEED)\n",
    "if len(filtered_images) < NUM_SAMPLES:\n",
    "    print(f'\\n‚ö†Ô∏è Warning: Only {len(filtered_images)} images available, requested {NUM_SAMPLES}')\n",
    "    sampled_images = filtered_images\n",
    "else:\n",
    "    sampled_images = random.sample(filtered_images, NUM_SAMPLES)\n",
    "    print(f'\\n‚úÖ Randomly selected {len(sampled_images)} images')\n",
    "\n",
    "# Create output folder and copy images\n",
    "OUTPUT_FOLDER.mkdir(parents=True, exist_ok=True)\n",
    "print(f'\\nCopying images to {OUTPUT_FOLDER}/...')\n",
    "\n",
    "copied_count = 0\n",
    "failed_count = 0\n",
    "for img_name in tqdm(sampled_images, desc='Copying images'):\n",
    "    src_path = DATASET_DIR / img_name\n",
    "    dst_path = OUTPUT_FOLDER / img_name\n",
    "    \n",
    "    try:\n",
    "        shutil.copy2(src_path, dst_path)\n",
    "        copied_count += 1\n",
    "    except Exception as e:\n",
    "        print(f'Failed to copy {img_name}: {e}')\n",
    "        failed_count += 1\n",
    "\n",
    "print(f'\\n‚úÖ Successfully copied {copied_count} images to {OUTPUT_FOLDER}/')\n",
    "if failed_count > 0:\n",
    "    print(f'‚ö†Ô∏è Failed to copy {failed_count} images')\n",
    "\n",
    "print(f'\\nFolder ready for upload to HPC server!')\n",
    "print(f'Total size: {sum(f.stat().st_size for f in OUTPUT_FOLDER.glob(\"*\") if f.is_file()) / (1024**2):.2f} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d109ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6471 images with allowed species labels\n",
      "Sampled 1200 images\n",
      "Created classes.txt with 7 classes\n",
      "Loading SAM...\n",
      "SAM loaded on CPU (slow)\n",
      "\n",
      "Starting processing with GPU acceleration...\n",
      "SAM loaded on CPU (slow)\n",
      "\n",
      "Starting processing with GPU acceleration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/1200 [00:03<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():  \u001b[38;5;66;03m# Critical for memory savings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         masks = \u001b[43mmask_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    131\u001b[39m     msg = \u001b[38;5;28mstr\u001b[39m(e).lower()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator.generate\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[32m    140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m \u001b[33;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m mask_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.min_mask_region_area > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._generate_masks\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    204\u001b[39m data = MaskData()\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     crop_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m     data.cat(crop_data)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/automatic_mask_generator.py:236\u001b[39m, in \u001b[36mSamAutomaticMaskGenerator._process_crop\u001b[39m\u001b[34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[39m\n\u001b[32m    234\u001b[39m cropped_im = image[y0:y1, x0:x1, :]\n\u001b[32m    235\u001b[39m cropped_im_size = cropped_im.shape[:\u001b[32m2\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_im\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;66;03m# Get points for this crop\u001b[39;00m\n\u001b[32m    239\u001b[39m points_scale = np.array(cropped_im_size)[\u001b[38;5;28;01mNone\u001b[39;00m, ::-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/predictor.py:60\u001b[39m, in \u001b[36mSamPredictor.set_image\u001b[39m\u001b[34m(self, image, image_format)\u001b[39m\n\u001b[32m     57\u001b[39m input_image_torch = torch.as_tensor(input_image, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     58\u001b[39m input_image_torch = input_image_torch.permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m).contiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/predictor.py:89\u001b[39m, in \u001b[36mSamPredictor.set_torch_image\u001b[39m\u001b[34m(self, transformed_image, original_image_size)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.input_size = \u001b[38;5;28mtuple\u001b[39m(transformed_image.shape[-\u001b[32m2\u001b[39m:])\n\u001b[32m     88\u001b[39m input_image = \u001b[38;5;28mself\u001b[39m.model.preprocess(transformed_image)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28mself\u001b[39m.features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mself\u001b[39m.is_image_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[39m, in \u001b[36mImageEncoderViT.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    109\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.pos_embed\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     x = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m x = \u001b[38;5;28mself\u001b[39m.neck(x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/segment_anything/modeling/image_encoder.py:168\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    167\u001b[39m     shortcut = x\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Window partition\u001b[39;00m\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.window_size > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Programming/wildLifeImageProcessing/wildlifeImageClassification/lib64/python3.13/site-packages/torch/nn/functional.py:2905\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2896\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2897\u001b[39m         layer_norm,\n\u001b[32m   2898\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2903\u001b[39m         eps=eps,\n\u001b[32m   2904\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2905\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2906\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2907\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Main processing cell: sample images, run SAM, select bbox closest to center, create YOLO labels\n",
    "\n",
    "# Ensure inputs\n",
    "if DOWNLOAD_CHECKPOINT and not SAM_CHECKPOINT.exists():\n",
    "    print(f'SAM checkpoint {SAM_CHECKPOINT} not found locally. Attempting download...')\n",
    "    try:\n",
    "        import urllib.request\n",
    "        SAM_CHECKPOINT.parent.mkdir(parents=True, exist_ok=True)\n",
    "        urllib.request.urlretrieve(SAM_VIT_B_URL, str(SAM_CHECKPOINT))\n",
    "        print('Downloaded SAM checkpoint')\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Failed to download SAM checkpoint: {e}')\n",
    "\n",
    "if not DATASET_DIR.exists():\n",
    "    raise FileNotFoundError(f'Dataset directory not found: {DATASET_DIR}')\n",
    "if not LABELS_JSON.exists():\n",
    "    raise FileNotFoundError(f'Labels JSON not found: {LABELS_JSON}')\n",
    "if not SAM_CHECKPOINT.exists():\n",
    "    raise FileNotFoundError(f'SAM checkpoint not found: {SAM_CHECKPOINT}')\n",
    "\n",
    "ensure_dir(OUTPUT_DIR)\n",
    "ensure_dir(OUTPUT_DIR / 'images')\n",
    "ensure_dir(OUTPUT_DIR / 'labels')\n",
    "ensure_dir(OUTPUT_DIR / 'vis')\n",
    "\n",
    "# Load labels JSON\n",
    "with open(LABELS_JSON, 'r', encoding='utf-8') as f:\n",
    "    image_categories = json.load(f)\n",
    "\n",
    "# Gather images\n",
    "all_images = [p.name for p in DATASET_DIR.iterdir() if p.is_file() and p.suffix.lower() in ['.jpg', '.jpeg', '.png']]\n",
    "available_images = [img for img in all_images if img in image_categories]\n",
    "if not available_images:\n",
    "    raise RuntimeError('No labeled images found in the dataset folder')\n",
    "\n",
    "# Filter to only allowed species\n",
    "filtered_images = []\n",
    "for img in available_images:\n",
    "    cats = image_categories.get(img)\n",
    "    if not cats:\n",
    "        continue\n",
    "    label = cats[0] if isinstance(cats, list) else cats\n",
    "    if label in ALLOWED_SPECIES:\n",
    "        filtered_images.append(img)\n",
    "\n",
    "print(f'Found {len(filtered_images)} images with allowed species labels')\n",
    "NUM_SAMPLES_ACTUAL = min(NUM_SAMPLES, len(filtered_images))\n",
    "random.seed(SEED)\n",
    "sampled = random.sample(filtered_images, NUM_SAMPLES_ACTUAL)\n",
    "print(f'Sampled {NUM_SAMPLES_ACTUAL} images')\n",
    "\n",
    "# map species to ids for classes.txt\n",
    "species_set = set()\n",
    "for img in sampled:\n",
    "    cats = image_categories.get(img)\n",
    "    if not cats:\n",
    "        continue\n",
    "    species_set.add(cats[0] if isinstance(cats, list) else cats)\n",
    "species_list = sorted(list(species_set))\n",
    "species_to_id = {s: i for i, s in enumerate(species_list)}\n",
    "with open(OUTPUT_DIR / 'classes.txt', 'w', encoding='utf-8') as f:\n",
    "    for s in species_list:\n",
    "        f.write(s + '\\n')\n",
    "print(f'Created classes.txt with {len(species_list)} classes')\n",
    "\n",
    "# Load SAM model\n",
    "print('Loading SAM...')\n",
    "try:\n",
    "    sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=str(SAM_CHECKPOINT))\n",
    "except KeyError:\n",
    "    raise KeyError(f'SAM model type {SAM_MODEL_TYPE} not found; available: {list(sam_model_registry.keys())}')\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f'Failed to load SAM: {e}')\n",
    "\n",
    "# Move to device - FORCE GPU or fail\n",
    "if SAM_DEVICE == 'cuda':\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError('CUDA requested but not available')\n",
    "    sam = sam.to(SAM_DEVICE)\n",
    "    print(f'SAM loaded on GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    sam = sam.to('cpu')\n",
    "    print('SAM loaded on CPU (slow)')\n",
    "\n",
    "# Create mask generator with PERFORMANCE-TUNED settings\n",
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    sam,\n",
    "    points_per_side=SAM_POINTS_PER_SIDE,  # Fewer points = faster (default 32)\n",
    "    pred_iou_thresh=SAM_PRED_IOU_THRESH,  # Higher = fewer low-quality masks\n",
    "    stability_score_thresh=SAM_STABILITY_SCORE_THRESH,  # Higher = fewer unstable masks\n",
    "    crop_n_layers=0,  # Disable crop layers for speed (default 0)\n",
    "    crop_n_points_downscale_factor=1,  # Not used if crop_n_layers=0\n",
    "    min_mask_region_area=100,  # Remove tiny masks\n",
    ")\n",
    "\n",
    "processed = 0\n",
    "skipped_oom = 0\n",
    "TOP_N_MASKS = 5  # Consider top 5 largest masks for center-based selection\n",
    "\n",
    "print(f'\\nStarting processing with GPU acceleration...')\n",
    "\n",
    "for img_name in tqdm(sampled, desc='Processing'):\n",
    "    src = DATASET_DIR / img_name\n",
    "    dst_img = OUTPUT_DIR / 'images' / img_name\n",
    "\n",
    "    img_bgr = cv2.imread(str(src))\n",
    "    if img_bgr is None:\n",
    "        print(f'Failed to read {src}; skipping')\n",
    "        continue\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    orig_h, orig_w = img_rgb.shape[:2]\n",
    "    resized = img_rgb\n",
    "    h, w = orig_h, orig_w\n",
    "\n",
    "    # ALWAYS downscale to save GPU memory\n",
    "    if MAX_IMAGE_LONG_SIDE is not None:\n",
    "        long_side = max(h, w)\n",
    "        if long_side > MAX_IMAGE_LONG_SIDE:\n",
    "            scale = MAX_IMAGE_LONG_SIDE / float(long_side)\n",
    "            new_w = int(w * scale)\n",
    "            new_h = int(h * scale)\n",
    "            resized = cv2.resize(img_rgb, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "            h, w = resized.shape[:2]\n",
    "\n",
    "    # Generate masks with GPU acceleration\n",
    "    try:\n",
    "        with torch.no_grad():  # Critical for memory savings\n",
    "            masks = mask_generator.generate(resized)\n",
    "    except RuntimeError as e:\n",
    "        msg = str(e).lower()\n",
    "        if 'out of memory' in msg or 'cuda' in msg:\n",
    "            skipped_oom += 1\n",
    "            print(f'\\nCUDA OOM on {img_name} (skipped {skipped_oom} total). Try reducing MAX_IMAGE_LONG_SIDE or SAM_POINTS_PER_SIDE')\n",
    "            torch.cuda.empty_cache()  # Clear cache and continue\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Failed to generate masks for {img_name}: {e}; skipping')\n",
    "            continue\n",
    "\n",
    "    if not masks:\n",
    "        print(f'No masks for {img_name}; skipping')\n",
    "        continue\n",
    "\n",
    "    # sort masks by area descending\n",
    "    masks_sorted = sorted(masks, key=lambda m: m.get('area', 0), reverse=True)\n",
    "    \n",
    "    # consider top N largest masks and pick the one closest to image center\n",
    "    candidates = masks_sorted[:TOP_N_MASKS]\n",
    "    \n",
    "    # build list of (distance_to_center, mask) tuples\n",
    "    candidate_distances = []\n",
    "    for mask in candidates:\n",
    "        bbox = mask.get('bbox')\n",
    "        if bbox is None:\n",
    "            seg = mask.get('segmentation')\n",
    "            ys, xs = np.where(seg)\n",
    "            if len(xs) == 0 or len(ys) == 0:\n",
    "                continue\n",
    "            x_min, x_max = int(xs.min()), int(xs.max())\n",
    "            y_min, y_max = int(ys.min()), int(ys.max())\n",
    "        else:\n",
    "            x_min, y_min, bw, bh = bbox\n",
    "            x_max = x_min + bw\n",
    "            y_max = y_min + bh\n",
    "        \n",
    "        # compute distance to image center (on resized coords)\n",
    "        dist = distance_to_image_center([x_min, y_min, x_max, y_max], w, h)\n",
    "        candidate_distances.append((dist, mask, [x_min, y_min, x_max, y_max]))\n",
    "    \n",
    "    if not candidate_distances:\n",
    "        print(f'No valid masks for {img_name}; skipping')\n",
    "        continue\n",
    "    \n",
    "    # pick the mask with minimum distance to center\n",
    "    candidate_distances.sort(key=lambda x: x[0])\n",
    "    best_dist, best_mask, best_bbox_resized = candidate_distances[0]\n",
    "    \n",
    "    # check area threshold\n",
    "    if best_mask.get('area', 0) < AREA_THRESHOLD * (h * w):\n",
    "        print(f'Best mask too small for {img_name}; skipping')\n",
    "        continue\n",
    "    \n",
    "    x_min, y_min, x_max, y_max = best_bbox_resized\n",
    "\n",
    "    # scale back to original image coords if resized\n",
    "    if (h, w) != (orig_h, orig_w):\n",
    "        scale_x = orig_w / float(w)\n",
    "        scale_y = orig_h / float(h)\n",
    "        x_min *= scale_x\n",
    "        x_max *= scale_x\n",
    "        y_min *= scale_y\n",
    "        y_max *= scale_y\n",
    "\n",
    "    yolo_box = xyxy_to_yolo([x_min, y_min, x_max, y_max], orig_w, orig_h)\n",
    "\n",
    "    cats = image_categories.get(img_name)\n",
    "    label_name = cats[0] if isinstance(cats, list) else cats\n",
    "    if label_name not in species_to_id:\n",
    "        print(f'Label {label_name} not in species mapping; skipping')\n",
    "        continue\n",
    "    class_id = species_to_id[label_name]\n",
    "\n",
    "    # save image and label\n",
    "    shutil.copy2(src, dst_img)\n",
    "    label_path = OUTPUT_DIR / 'labels' / (img_name.rsplit('.', 1)[0] + '.txt')\n",
    "    with open(label_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"{class_id} {' '.join([f'{v:.6f}' for v in yolo_box])}\\n\")\n",
    "\n",
    "    # save visualization with bbox and center crosshair\n",
    "    vis = img_rgb.copy()\n",
    "    # draw selected bbox in green\n",
    "    cv2.rectangle(vis, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 3)\n",
    "    # draw crosshair at image center in blue\n",
    "    center_x = int(orig_w / 2)\n",
    "    center_y = int(orig_h / 2)\n",
    "    cv2.line(vis, (center_x - 20, center_y), (center_x + 20, center_y), (0, 0, 255), 2)\n",
    "    cv2.line(vis, (center_x, center_y - 20), (center_x, center_y + 20), (0, 0, 255), 2)\n",
    "    cv2.imwrite(str(OUTPUT_DIR / 'vis' / img_name), cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    processed += 1\n",
    "    if processed % 10 == 0:\n",
    "        print(f'Processed {processed}/{NUM_SAMPLES_ACTUAL}')\n",
    "\n",
    "print(f'\\nDone! Processed {processed} images. Skipped {skipped_oom} due to OOM.')\n",
    "print(f'Output at: {OUTPUT_DIR}')\n",
    "if SAM_DEVICE == 'cuda':\n",
    "    print(f'Peak GPU memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5c210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891fc915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b169c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867a604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf344e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b109350b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f858e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1022d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de142abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b9b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlifeImageClassification (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
